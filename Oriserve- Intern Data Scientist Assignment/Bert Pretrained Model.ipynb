{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ee290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78aaa8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence', 'Col_1', 'Col_2', 'Col_3', 'Col_4', 'Col_5', 'Col_6',\n",
       "       'Col_7', 'Col_8', 'Col_9', 'Col_10', 'Col_11', 'Col_12', 'Col_13',\n",
       "       'Col_14'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('../Oriserve- Intern Data Scientist Assignment/Evaluation-dataset.csv', header=None)\n",
    "\n",
    "sentiment_array = ['Col_'+ str(i) for i in range(1,15)]\n",
    "\n",
    "new_col = ['Sentence'] + sentiment_array\n",
    "\n",
    "data.columns = new_col\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc6bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequencies of sentiment labels\n",
    "sent = {}\n",
    "for col in data.columns:\n",
    "    if col != 'Sentence':\n",
    "        z = data[col].dropna()\n",
    "        for label in z:\n",
    "            sent[label] = sent.get(label, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c89b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sentiment labels with frequency greater than 30\n",
    "list_1 = [key for key, value in sent.items() if value > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78dcf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify data to include only selected sentiment labels\n",
    "def dataModification(data, selected_labels):\n",
    "    data1 = data.iloc[:, 1:]\n",
    "    data2 = pd.DataFrame(index=range(0, data1.shape[0]), columns=selected_labels)\n",
    "    for i, row in data1.iterrows():\n",
    "        subthemes = row.dropna().tolist()\n",
    "        for label in subthemes:\n",
    "            if label in selected_labels:\n",
    "                data2.loc[i][label] = 1\n",
    "    data2 = data2.fillna(0)\n",
    "    final_data = pd.DataFrame()\n",
    "    final_data['Sentence'] = data['Sentence']\n",
    "    for column in data2.columns:\n",
    "        final_data[column] = data2[column]\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6057f84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivek Ghodmare\\AppData\\Local\\Temp\\ipykernel_5808\\1278078272.py:9: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data2.loc[i][label] = 1\n",
      "C:\\Users\\Vivek Ghodmare\\AppData\\Local\\Temp\\ipykernel_5808\\1278078272.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data2 = data2.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "final_data = dataModification(data, list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30099ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea9713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['Sent_Processed'] = final_data['Sentence'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6dc0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_data['Sent_Processed'], final_data[list_1], test_size=0.2)# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_data['Sent_Processed'], final_data[list_1], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a6ecf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502a80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input text\n",
    "def tokenize_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c20632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = X_train.apply(tokenize_text)\n",
    "X_test_tokenized = X_test.apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4fcff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Filter out non-numeric values from y_train\n",
    "y_train_numeric = np.array([float(label) for label in y_train if isinstance(label, str) and label.replace('.', '').isdigit()])\n",
    "\n",
    "# Train the BERT model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(3):  # Train for 3 epochs, you can adjust this\n",
    "    model.train()\n",
    "    for inputs, label in zip(X_train_tokenized, y_train_numeric):  # Use y_train_numeric\n",
    "        inputs = {key: torch.tensor(val).to(device) for key, val in inputs.items()}\n",
    "        label = torch.tensor(label, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8ddd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivek Ghodmare\\AppData\\Local\\Temp\\ipykernel_5808\\1627715325.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = {key: torch.tensor(val).to(device) for key, val in inputs.items()}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the BERT model\n",
    "model.eval()\n",
    "y_pred_probs = []\n",
    "for inputs in X_test_tokenized:\n",
    "    inputs = {key: torch.tensor(val).to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    y_pred_probs.append(outputs.logits.cpu().numpy())\n",
    "\n",
    "y_pred_probs = np.array(y_pred_probs)\n",
    "y_pred_class = (y_pred_probs > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e70c8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c03a819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.006413418845584608\n",
      "F1-Score: 0.007655502392344498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       406\n",
      "           1       0.00      0.00      0.00       930\n",
      "           2       0.00      0.00      0.00       254\n",
      "           3       0.00      0.00      0.00       233\n",
      "           4       0.00      0.00      0.00       130\n",
      "           5       0.00      0.00      0.00        31\n",
      "           6       0.00      0.00      0.00        84\n",
      "           7       0.00      0.00      0.00        88\n",
      "           8       0.00      0.00      0.00        21\n",
      "           9       0.00      0.00      0.00        93\n",
      "          10       0.00      0.00      0.00        53\n",
      "          11       0.00      0.00      0.00        27\n",
      "          12       0.00      0.00      0.00        23\n",
      "          13       0.00      0.00      0.00        22\n",
      "          14       0.00      0.00      0.00        53\n",
      "          15       0.00      0.00      0.00        33\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.60      0.00         5\n",
      "          18       0.01      0.86      0.01         7\n",
      "          19       0.00      0.00      0.00        13\n",
      "          20       0.01      0.43      0.02        23\n",
      "          21       0.00      0.00      0.00        45\n",
      "          22       0.00      0.00      0.00        52\n",
      "          23       0.00      0.00      0.00        20\n",
      "          24       0.00      0.00      0.00        43\n",
      "          25       0.00      0.00      0.00         6\n",
      "          26       0.00      0.00      0.00        22\n",
      "          27       0.00      0.00      0.00         6\n",
      "          28       0.00      0.00      0.00         5\n",
      "          29       0.00      0.00      0.00        13\n",
      "          30       0.00      0.00      0.00        10\n",
      "          31       0.01      0.68      0.02        19\n",
      "          32       0.00      0.00      0.00         9\n",
      "\n",
      "   micro avg       0.01      0.01      0.01      2782\n",
      "   macro avg       0.00      0.08      0.00      2782\n",
      "weighted avg       0.00      0.01      0.00      2782\n",
      " samples avg       0.01      0.01      0.01      2782\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivek Ghodmare\\anaconda3\\envs\\dsml28_env1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Vivek Ghodmare\\anaconda3\\envs\\dsml28_env1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Vivek Ghodmare\\anaconda3\\envs\\dsml28_env1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Vivek Ghodmare\\anaconda3\\envs\\dsml28_env1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Convert y_pred_probs to a single NumPy array\n",
    "y_pred_probs = np.vstack(y_pred_probs)\n",
    "\n",
    "# If necessary, convert probabilities to binary predictions (assuming a threshold of 0.5)\n",
    "y_pred_class = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Ensure y_pred_class has the same shape as y_test.values\n",
    "if y_pred_class.shape != y_test.values.shape:\n",
    "    raise ValueError(\"Shape mismatch: y_pred_class and y_test.values must have the same shape.\")\n",
    "\n",
    "# Evaluate performance\n",
    "ac = accuracy_score(y_test.values, y_pred_class)\n",
    "f1 = f1_score(y_test.values, y_pred_class, average='micro')\n",
    "print('Accuracy Score:', ac)\n",
    "print('F1-Score:', f1)\n",
    "print(classification_report(y_test.values, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece27e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsml28_env1]",
   "language": "python",
   "name": "conda-env-dsml28_env1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
